{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "202ed1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import imageio\n",
    "from skimage import transform,io\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout, UpSampling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3174a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(labels, class_name, index):\n",
    "    dummy_arr = np.zeros((len(labels)))\n",
    "    dummy_arr[index] += 1\n",
    "    return dummy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cadefae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data():\n",
    "    N = 0\n",
    "    labels = []\n",
    "\n",
    "    for dirname, _, filenames in os.walk('./train/'):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(dirname, filename)\n",
    "            splits = dirname.split('/')\n",
    "            N += 1\n",
    "            class_name = splits[-1]\n",
    "\n",
    "            if class_name not in labels and class_name != '': labels.append(splits[-1])\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    im_size = 224\n",
    "\n",
    "    X1 = np.zeros((N, im_size, im_size, 3))\n",
    "    y1 = np.zeros((N, len(labels)))\n",
    "\n",
    "    for dirname, _, filenames in os.walk('./train/'):\n",
    "        for filename in filenames:\n",
    "            if filename == '.DS_Store': continue\n",
    "\n",
    "            path = os.path.join(dirname, filename)        \n",
    "            l = path.split('/')[-2]\n",
    "\n",
    "            im = imageio.imread(path)/255. # normalize images\n",
    "            X1[count, :, :] = transform.resize(im, (im_size, im_size, 3), mode='symmetric', preserve_range=True) # downsampling original image\n",
    "            y1[count, :] = get_dummies(labels, l, np.where(labels == l)[0][0])\n",
    "\n",
    "            count +=1\n",
    "       \n",
    "    # create data generator\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True, # horizontal flip\n",
    "            brightness_range=[0.9,1.3]) # brightness\n",
    "\n",
    "    # fit parameters from data\n",
    "    datagen.fit(X1)\n",
    "\n",
    "    X2 = np.zeros((N, im_size, im_size, 3))\n",
    "    y2 = np.zeros((N, len(labels)))\n",
    "\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "    for X_batch, y_batch in datagen.flow(X1, y1, batch_size=N):\n",
    "        for i in range(0, N, 1):\n",
    "            X2[i,:,:,:] = X_batch[i].reshape(im_size,im_size,3)/255.\n",
    "            y2[i] = y_batch[i]\n",
    "        # De esta manera paramos de generar imagenes aleatoriamente\n",
    "        break\n",
    "        \n",
    "    X3 = np.zeros((N, im_size, im_size, 3))\n",
    "    y3 = np.zeros((N, len(labels)))\n",
    "\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "    for X_batch, y_batch in datagen.flow(X1, y1, batch_size=N):\n",
    "        for i in range(0, N, 1):\n",
    "            X3[i,:,:,:] = X_batch[i].reshape(im_size,im_size,3)/255.\n",
    "            y3[i] = y_batch[i]\n",
    "        # De esta manera paramos de generar imagenes aleatoriamente\n",
    "        break\n",
    "        \n",
    "    X4 = np.zeros((N, im_size, im_size, 3))\n",
    "    y4 = np.zeros((N, len(labels)))\n",
    "\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "    for X_batch, y_batch in datagen.flow(X1, y1, batch_size=N):\n",
    "        for i in range(0, N, 1):\n",
    "            X4[i,:,:,:] = X_batch[i].reshape(im_size,im_size,3)/255.\n",
    "            y4[i] = y_batch[i]\n",
    "        # De esta manera paramos de generar imagenes aleatoriamente\n",
    "        break\n",
    "        \n",
    "    X5 = np.zeros((N, im_size, im_size, 3))\n",
    "    y5 = np.zeros((N, len(labels)))\n",
    "\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "    for X_batch, y_batch in datagen.flow(X1, y1, batch_size=N):\n",
    "        for i in range(0, N, 1):\n",
    "            X5[i,:,:,:] = X_batch[i].reshape(im_size,im_size,3)/255.\n",
    "            y5[i] = y_batch[i]\n",
    "        # De esta manera paramos de generar imagenes aleatoriamente\n",
    "        break\n",
    "        \n",
    "    X6 = np.zeros((N, im_size, im_size, 3))\n",
    "    y6 = np.zeros((N, len(labels)))\n",
    "\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "    for X_batch, y_batch in datagen.flow(X1, y1, batch_size=N):\n",
    "        for i in range(0, N, 1):\n",
    "            X6[i,:,:,:] = X_batch[i].reshape(im_size,im_size,3)/255.\n",
    "            y6[i] = y_batch[i]\n",
    "        # De esta manera paramos de generar imagenes aleatoriamente\n",
    "        break\n",
    "        \n",
    "    X7 = np.zeros((N, im_size, im_size, 3))\n",
    "    y7 = np.zeros((N, len(labels)))\n",
    "\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "    for X_batch, y_batch in datagen.flow(X1, y1, batch_size=N):\n",
    "        for i in range(0, N, 1):\n",
    "            X7[i,:,:,:] = X_batch[i].reshape(im_size,im_size,3)/255.\n",
    "            y7[i] = y_batch[i]\n",
    "        # De esta manera paramos de generar imagenes aleatoriamente\n",
    "        break\n",
    "        \n",
    "    X8 = np.zeros((N, im_size, im_size, 3))\n",
    "    y8 = np.zeros((N, len(labels)))\n",
    "\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "    for X_batch, y_batch in datagen.flow(X1, y1, batch_size=N):\n",
    "        for i in range(0, N, 1):\n",
    "            X8[i,:,:,:] = X_batch[i].reshape(im_size,im_size,3)/255.\n",
    "            y8[i] = y_batch[i]\n",
    "        # De esta manera paramos de generar imagenes aleatoriamente\n",
    "        break\n",
    "        \n",
    "    X9 = np.zeros((N, im_size, im_size, 3))\n",
    "    y9 = np.zeros((N, len(labels)))\n",
    "\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "    for X_batch, y_batch in datagen.flow(X1, y1, batch_size=N):\n",
    "        for i in range(0, N, 1):\n",
    "            X9[i,:,:,:] = X_batch[i].reshape(im_size,im_size,3)/255.\n",
    "            y9[i] = y_batch[i]\n",
    "        # De esta manera paramos de generar imagenes aleatoriamente\n",
    "        break\n",
    "        \n",
    "    X10 = np.zeros((N, im_size, im_size, 3))\n",
    "    y10 = np.zeros((N, len(labels)))\n",
    "\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "    for X_batch, y_batch in datagen.flow(X1, y1, batch_size=N):\n",
    "        for i in range(0, N, 1):\n",
    "            X10[i,:,:,:] = X_batch[i].reshape(im_size,im_size,3)/255.\n",
    "            y10[i] = y_batch[i]\n",
    "        # De esta manera paramos de generar imagenes aleatoriamente\n",
    "        break\n",
    "    \n",
    "    X = np.concatenate((X1,X2,X3,X4,X5,X6,X7,X8,X9,X10))\n",
    "    y = np.concatenate((y1,y2,y3,y4,y5,y6,y7,y8,y9,y10))\n",
    "    \n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    print(\"X_train shape: \", X_train.shape)\n",
    "    print(\"y_train shape: \", y_train.shape)\n",
    "    print(\"X_val shape: \", X_val.shape)\n",
    "    print(\"y_val shape: \", y_val.shape)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5da9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Learning rate schedulers\n",
    "class LearningRateDecay:\n",
    "    def plot(self, epochs, title=\"Learning Rate Schedule\"):\n",
    "        # compute the set of learning rates for each corresponding\n",
    "        # epoch\n",
    "        lrs = [self(i) for i in epochs]\n",
    "        # the learning rate schedule\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, lrs)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        \n",
    "class StepDecay(LearningRateDecay):\n",
    "    def __init__(self, initAlpha=1e-5, factor=0.5, dropEvery=10):\n",
    "        # store the base initial learning rate, drop factor, and\n",
    "        # epochs to drop every\n",
    "        self.initAlpha = initAlpha\n",
    "        self.factor = factor\n",
    "        self.dropEvery = dropEvery\n",
    "        \n",
    "    def __call__(self, epoch):\n",
    "        # compute the learning rate for the current epoch\n",
    "        exp = np.floor((1 + epoch) / self.dropEvery)\n",
    "        alpha = self.initAlpha * (self.factor ** exp)\n",
    "        \n",
    "        # return the learning rate\n",
    "        return float(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6116508",
   "metadata": {},
   "source": [
    "### Autoencoder for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09b5c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "## With a Neural Network\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c0edf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = keras.Sequential([\n",
    "    # Encoder\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=[224, 224, 3]),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    \n",
    "    # Decoder\n",
    "\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.UpSampling2D((2, 2)),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.UpSampling2D((2, 2)),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.UpSampling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.UpSampling2D((2, 2)),\n",
    "    keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1091fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "439c06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-3\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(X_train)//batch_size\n",
    "validation_steps = len(X_val)//batch_size\n",
    "\n",
    "mc_auto = ModelCheckpoint(\"autoencoderX_v3.h5\", verbose=1, save_best_only=True)\n",
    "es_auto = EarlyStopping(patience=5, verbose=1)\n",
    "lr_s = LearningRateScheduler(StepDecay(initAlpha=initial_lr), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4774ec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 32)      18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 224, 224, 3)       1731      \n",
      "=================================================================\n",
      "Total params: 86,723\n",
      "Trainable params: 86,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "940f3c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (8715, 224, 224, 3)\n",
      "y_train shape:  (8715, 11)\n",
      "X_val shape:  (2905, 224, 224, 3)\n",
      "y_val shape:  (2905, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = load_and_split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b062433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 21:08:24.263957: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.1198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 21:11:09.007228: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 183s 670ms/step - loss: 0.1387 - accuracy: 0.1198 - val_loss: 0.5390 - val_accuracy: 0.5564\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53896, saving model to autoencoderX_v3.h5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 173s 636ms/step - loss: 0.0962 - accuracy: 0.1395 - val_loss: 0.5111 - val_accuracy: 0.7302\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53896 to 0.51108, saving model to autoencoderX_v3.h5\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 172s 633ms/step - loss: 0.0938 - accuracy: 0.1378 - val_loss: 0.5109 - val_accuracy: 0.7666\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51108 to 0.51089, saving model to autoencoderX_v3.h5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 174s 639ms/step - loss: 0.0947 - accuracy: 0.1546 - val_loss: 0.5068 - val_accuracy: 0.7625\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.51089 to 0.50682, saving model to autoencoderX_v3.h5\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 174s 639ms/step - loss: 0.0936 - accuracy: 0.1839 - val_loss: 0.5058 - val_accuracy: 0.7344\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50682 to 0.50582, saving model to autoencoderX_v3.h5\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 288s 1s/step - loss: 0.0928 - accuracy: 0.2242 - val_loss: 0.5050 - val_accuracy: 0.7525\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.50582 to 0.50498, saving model to autoencoderX_v3.h5\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 175s 642ms/step - loss: 0.0930 - accuracy: 0.4539 - val_loss: 0.5074 - val_accuracy: 0.7473\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50498\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 174s 641ms/step - loss: 0.0935 - accuracy: 0.4270 - val_loss: 0.5064 - val_accuracy: 0.7332\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.50498\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 174s 638ms/step - loss: 0.0934 - accuracy: 0.4342 - val_loss: 0.5060 - val_accuracy: 0.7591\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.50498\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n",
      "272/272 [==============================] - 174s 639ms/step - loss: 0.0933 - accuracy: 0.8018 - val_loss: 0.5030 - val_accuracy: 0.7830\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.50498 to 0.50300, saving model to autoencoderX_v3.h5\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n",
      "272/272 [==============================] - 215s 791ms/step - loss: 0.0921 - accuracy: 0.8679 - val_loss: 0.5023 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50300 to 0.50234, saving model to autoencoderX_v3.h5\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n",
      "272/272 [==============================] - 841s 3s/step - loss: 0.0921 - accuracy: 0.8542 - val_loss: 0.5022 - val_accuracy: 0.7514\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.50234 to 0.50223, saving model to autoencoderX_v3.h5\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n",
      "272/272 [==============================] - 1365s 5s/step - loss: 0.0926 - accuracy: 0.8775 - val_loss: 0.5053 - val_accuracy: 0.7607\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.50223\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n",
      "272/272 [==============================] - 775s 3s/step - loss: 0.0932 - accuracy: 0.8831 - val_loss: 0.5015 - val_accuracy: 0.7707\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.50223 to 0.50146, saving model to autoencoderX_v3.h5\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.8468"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenaremos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmc_auto\u001b[49m\u001b[43m,\u001b[49m\u001b[43mes_auto\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr_s\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/training.py:1215\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1202\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1203\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1204\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1214\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> 1215\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1228\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/training.py:1501\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1500\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1501\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1503\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:924\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    926\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    927\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entrenaremos\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_data=(X_val, X_val),\n",
    "                validation_steps=validation_steps,\n",
    "                callbacks=[mc_auto,es_auto,lr_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving training history (for future visualization)\n",
    "with open('./train_history_autoencoderX_v3.pkl', 'wb') as handle:\n",
    "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cffa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'accuracy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab256f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructions(model, images=X_val, n_images=5):\n",
    "    reconstructions = model.predict(images[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images * 3, 6))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plt.imshow(images[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plt.imshow(reconstructions[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(autoencoder, images=X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cfff8",
   "metadata": {},
   "source": [
    "### Paper architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beb00f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-3\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(X_train)//batch_size\n",
    "validation_steps = len(X_val)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0210a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(224, 224, 3), filters=96, kernel_size=(11,11), padding='same', activation=activation))\n",
    "model.add(MaxPool2D(pool_size=(3,3)))\n",
    "model.add(Conv2D(filters=96, kernel_size=(5,5), padding='same', activation=activation))\n",
    "model.add(MaxPool2D(pool_size=(3,3)))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation=activation))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation=activation))\n",
    "model.add(MaxPool2D(pool_size=(3,3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=4096,activation=activation))\n",
    "model.add(Dense(units=11, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a7312a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using accuracy to measure model performance\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=initial_lr, momentum=0.9), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81f70a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "mc = ModelCheckpoint(\"cloudXnet_v2.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "es = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "lr_s = LearningRateScheduler(StepDecay(initAlpha=initial_lr, factor=0.1, dropEvery=50), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8518bd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 19:13:14.559614: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - ETA: 0s - loss: 2.3794 - accuracy: 0.1344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 19:16:03.605154: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 184s 673ms/step - loss: 2.3794 - accuracy: 0.1344 - val_loss: 2.3347 - val_accuracy: 0.1580\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.15799, saving model to cloudXnet_v2.h5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 178s 657ms/step - loss: 2.3643 - accuracy: 0.1384 - val_loss: 2.3096 - val_accuracy: 0.1562\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.15799\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 177s 653ms/step - loss: 2.3566 - accuracy: 0.1423 - val_loss: 2.2541 - val_accuracy: 0.2066\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.15799 to 0.20660, saving model to cloudXnet_v2.h5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 177s 652ms/step - loss: 2.3470 - accuracy: 0.1452 - val_loss: 2.1781 - val_accuracy: 0.1944\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.20660\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 454s 2s/step - loss: 2.3388 - accuracy: 0.1520 - val_loss: 2.1546 - val_accuracy: 0.2010\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.20660\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 177s 652ms/step - loss: 2.3307 - accuracy: 0.1524 - val_loss: 2.1187 - val_accuracy: 0.2677\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.20660 to 0.26771, saving model to cloudXnet_v2.h5\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 177s 652ms/step - loss: 2.3246 - accuracy: 0.1570 - val_loss: 2.0946 - val_accuracy: 0.2545\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.26771\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 178s 653ms/step - loss: 2.3176 - accuracy: 0.1618 - val_loss: 2.0610 - val_accuracy: 0.2788\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.26771 to 0.27882, saving model to cloudXnet_v2.h5\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 177s 653ms/step - loss: 2.3105 - accuracy: 0.1619 - val_loss: 2.0543 - val_accuracy: 0.2892\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.27882 to 0.28924, saving model to cloudXnet_v2.h5\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 177s 652ms/step - loss: 2.3081 - accuracy: 0.1639 - val_loss: 2.0396 - val_accuracy: 0.2917\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.28924 to 0.29167, saving model to cloudXnet_v2.h5\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 239s 880ms/step - loss: 2.3037 - accuracy: 0.1631 - val_loss: 2.0518 - val_accuracy: 0.2837\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.29167\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 177s 652ms/step - loss: 2.2992 - accuracy: 0.1665 - val_loss: 2.0172 - val_accuracy: 0.3000\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.29167 to 0.30000, saving model to cloudXnet_v2.h5\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 178s 655ms/step - loss: 2.2961 - accuracy: 0.1678 - val_loss: 2.0087 - val_accuracy: 0.2997\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.30000\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 178s 653ms/step - loss: 2.2907 - accuracy: 0.1683 - val_loss: 2.0134 - val_accuracy: 0.3031\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.30000 to 0.30312, saving model to cloudXnet_v2.h5\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 178s 653ms/step - loss: 2.2870 - accuracy: 0.1706 - val_loss: 2.0189 - val_accuracy: 0.2972\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.30312\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 655s 2s/step - loss: 2.2829 - accuracy: 0.1752 - val_loss: 1.9702 - val_accuracy: 0.3191\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.30312 to 0.31910, saving model to cloudXnet_v2.h5\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 668s 2s/step - loss: 2.2781 - accuracy: 0.1777 - val_loss: 1.9779 - val_accuracy: 0.3087\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.31910\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 341s 1s/step - loss: 2.2788 - accuracy: 0.1730 - val_loss: 1.9605 - val_accuracy: 0.3281\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.31910 to 0.32812, saving model to cloudXnet_v2.h5\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 539s 2s/step - loss: 2.2700 - accuracy: 0.1787 - val_loss: 1.9476 - val_accuracy: 0.3285\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.32812 to 0.32847, saving model to cloudXnet_v2.h5\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 190s 698ms/step - loss: 2.2679 - accuracy: 0.1784 - val_loss: 1.9595 - val_accuracy: 0.3313\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.32847 to 0.33125, saving model to cloudXnet_v2.h5\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 325s 1s/step - loss: 2.2620 - accuracy: 0.1817 - val_loss: 1.9403 - val_accuracy: 0.3326\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.33125 to 0.33264, saving model to cloudXnet_v2.h5\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - 177s 652ms/step - loss: 2.2534 - accuracy: 0.1815 - val_loss: 1.9350 - val_accuracy: 0.3319\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.33264\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "272/272 [==============================] - ETA: 0s - loss: 2.2510 - accuracy: 0.1869"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr_s\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# saving training history (for future visualization)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./train_history_cloudXnet_v2.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/training.py:1215\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1202\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1203\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1204\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1214\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> 1215\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1228\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/training.py:1501\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1500\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1501\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1503\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:924\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    926\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    927\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[mc,es,lr_s])\n",
    "\n",
    "# saving training history (for future visualization)\n",
    "with open('./train_history_cloudXnet_v2.pkl', 'wb') as handle:\n",
    "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423f28b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
